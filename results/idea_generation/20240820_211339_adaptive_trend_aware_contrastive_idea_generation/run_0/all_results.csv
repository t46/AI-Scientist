paper_id,Summary,Questions,Limitations,Ethical Concerns,Soundness,Presentation,Contribution,Overall,Confidence,Strengths,Weaknesses,Originality,Quality,Clarity,Significance,Decision
20240820_170715_attention_mechanism,"The paper proposes integrating self-attention mechanisms into diffusion models to address the issue of mode collapse. The authors introduce a SelfAttention module and integrate it into the MLPDenoiser architecture. They validate their approach on various 2D datasets, showing improvements in evaluation loss, KL divergence, and sample quality compared to baseline models.","['Can the authors provide more details on the implementation of the SelfAttention module and its integration into the MLPDenoiser architecture?', 'How does the proposed method compare with other state-of-the-art methods?', 'What are the specific reasons for the limited improvement on certain datasets?', 'How do the self-attention layers impact the computational overhead, and are there any strategies to mitigate this overhead?', 'Can the authors provide more theoretical or empirical justification for why self-attention specifically improves mode capture in diffusion models?']","['The paper does not adequately address the computational overhead introduced by the self-attention layers.', ""The experimental validation is limited to simple 2D datasets, which may not fully demonstrate the method's effectiveness in more complex, high-dimensional data."", 'There is no discussion on the societal impact of the work.']",False,2,2,2,3,4,"['Addresses the important issue of mode collapse in generative models.', 'Proposes a novel application of self-attention mechanisms in diffusion models.', 'The experimental results show some improvements in evaluation metrics, indicating the potential effectiveness of the approach.']","['The methodology section lacks sufficient detail, particularly in the description of the SelfAttention module and its integration into the MLPDenoiser architecture.', 'The experimental validation is limited to 2D datasets and lacks comparison with a broader set of state-of-the-art methods.', 'The paper does not provide sufficient theoretical insights or detailed analysis to explain why self-attention improves mode capture in diffusion models.', 'The impact of the self-attention layers on computational overhead is not adequately discussed.', 'The paper lacks clarity in several sections, making it difficult to follow the proposed approach and its contributions.']",3,2,2,2,Reject
