paper_id,Summary,Questions,Limitations,Ethical Concerns,Soundness,Presentation,Contribution,Overall,Confidence,Strengths,Weaknesses,Originality,Quality,Clarity,Significance,Decision
20240820_103614_learning_rate_schedule,"The paper investigates the impact of various adaptive learning rate schedules on the performance of diffusion models, focusing on improving training efficiency and sample quality. The study compares different learning rate schedules, including StepLR, ExponentialLR, ReduceLROnPlateau, and CyclicLR, using the final estimated KL divergence as the primary metric. Experiments are conducted on multiple 2D datasets.","['Why were only 2D datasets used for the experiments? How do you plan to extend this to higher-dimensional datasets?', 'Can you provide more detailed descriptions of the adaptive learning rate schedules and their parameter settings?', 'Have you considered conducting statistical significance tests to validate the results?', 'Can the authors provide more details about the autoencoder aggregator?', 'How were the hyperparameters for each learning rate schedule chosen?', 'Can the authors provide more visual examples for each dataset and learning rate schedule to better illustrate the differences?', 'Can the authors provide more in-depth analysis on why certain learning rate schedules perform better than others?', 'What are the potential limitations when applying the findings to higher-dimensional datasets or more complex generative tasks?', 'Can the authors provide more detailed explanations of the diffusion model architecture and the noise scheduler used in the experiments?', 'What are the specific hyperparameters for the adaptive learning rate schedules used in the experiments?', 'Can the authors include more qualitative analysis and visualizations to support the quantitative results presented?']","['The study is limited to 2D datasets, which restricts the generalizability of the findings.', 'The choice of hyperparameters such as the base learning rate and batch size may influence the results.', 'The paper does not convincingly argue how these insights can be generalized to more complex datasets or real-world applications.', 'The limitations section does not adequately address the concerns about the simplistic nature of the datasets and the minimal improvements observed.']",False,2,2,2,3,4,"['Addresses an important aspect of training diffusion models: the choice of learning rate schedule.', 'Provides a comparative study of different adaptive learning rate schedules.', 'Includes multiple evaluation metrics to assess the performance.']","['Experiments are conducted only on simple 2D datasets, limiting the generalizability of the findings.', 'Results are inconclusive and do not show a clear advantage of one learning rate schedule over the others.', 'Lacks a deep theoretical contribution or novel methodological insights.', 'The description of the experimental setup and the adaptive learning rate schedules could be more detailed.', 'Figures are not very informative and do not add much value to the paper.', 'The related work section does not adequately cover the most recent advancements in adaptive learning rate schedules and their applications in diffusion models.', 'The clarity of the paper is compromised by the lack of detailed explanations for certain methodologies, such as the specific implementation of the diffusion model architecture and the noise scheduler.', 'The results section does not provide enough qualitative analysis or visualizations to support the quantitative metrics presented.']",2,2,2,2,Reject
