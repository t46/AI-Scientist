[
    {
        "Name": "random_fourier_features",
        "Title": "Exploring Random Fourier Features as Embeddings for Diffusion Models",
        "Method": "Replace the current sinusoidal embeddings with Random Fourier Features (RFF) embeddings. RFF approximates Gaussian RBF kernels, capturing non-linear structures in low-dimensional data effectively.",
        "Experiment": "Implement a new embedding class `RandomFourierEmbedding` and replace the `SinusoidalEmbedding` in the MLPDenoiser. Train the model using the same datasets and compare performance metrics such as eval loss, KL divergence, and inference time with the baseline sinusoidal embeddings.",
        "Interestingness": 8,
        "Feasibility": 9,
        "Novelty": 7,
        "Significance": 6
    },
    {
        "Name": "mode_conditioning",
        "Title": "Mode-Conditioned Diffusion Models for Controllable Generation",
        "Method": "Introduce a mode-conditioning variable to the diffusion model to allow controllable generation towards different modes of the data. This variable will be a one-hot encoded vector indicating the desired mode, and it will be concatenated with the input features and time embeddings before being fed into the MLPDenoiser. For datasets with distinct modes, mode labels will be determined using clustering algorithms such as K-means.",
        "Experiment": "1. Implement a new class ModeConditionedMLPDenoiser that extends MLPDenoiser to take a mode-conditioning variable as an additional input. 2. Use K-means clustering to determine mode labels for the datasets. 3. Modify the training loop to include mode labels as additional inputs. 4. Train the model on the provided datasets with mode-conditioning. 5. Evaluate the model by generating samples conditioned on different modes and comparing the distribution of generated samples with the true data distribution. Metrics to be used include eval loss, KL divergence, and visual inspection of generated samples.",
        "Interestingness": 9,
        "Feasibility": 8,
        "Novelty": 8,
        "Significance": 7
    },
    {
        "Name": "learned_noise_schedule",
        "Title": "Adaptive Noise Schedules: Learning Optimal Noise Distributions for Diffusion Models",
        "Method": "Introduce a LearnedNoiseScheduler class where noise parameters (betas) are initialized as learnable tensors. These parameters are optimized during training alongside the model parameters. This allows the model to adaptively learn the optimal noise distribution for the given data.",
        "Experiment": "1. Implement a new class LearnedNoiseScheduler that extends NoiseScheduler to have learnable noise parameters. 2. Modify the current training loop to include optimization of the noise parameters. 3. Train the model on the provided datasets using the learned noise schedule. 4. Compare the learned noise schedule with the fixed linear and quadratic schedules in terms of eval loss, KL divergence, and inference time.",
        "Interestingness": 9,
        "Feasibility": 7,
        "Novelty": 8,
        "Significance": 8
    },
    {
        "Name": "spherical_harmonic_embeddings",
        "Title": "Exploring Spherical Harmonic Embeddings for Diffusion Models",
        "Method": "Replace the current sinusoidal embeddings with Spherical Harmonic Embeddings (SHE). Spherical Harmonics can capture geometric invariances and symmetries in low-dimensional data more effectively than sinusoidal embeddings.",
        "Experiment": "1. Implement a new embedding class `SphericalHarmonicEmbedding` that generates embeddings based on spherical harmonics. 2. Replace the `SinusoidalEmbedding` in the `MLPDenoiser` with `SphericalHarmonicEmbedding`. 3. Train the model using the same datasets and compare performance metrics such as eval loss, KL divergence, and inference time with the baseline sinusoidal embeddings.",
        "Interestingness": 9,
        "Feasibility": 8,
        "Novelty": 8,
        "Significance": 7
    },
    {
        "Name": "graph_neural_network_embeddings",
        "Title": "Enhancing Diffusion Models with Graph Neural Network Embeddings",
        "Method": "Replace the current sinusoidal embeddings with embeddings generated by a Graph Neural Network (GNN). Construct a graph using k-Nearest Neighbors (k-NN) where each data point is a node, and edges represent similarities between points. Use a specific GNN model, such as a Graph Convolutional Network (GCN), to propagate information through this graph and generate embeddings that encapsulate the structural relationships in the data.",
        "Experiment": "1. Implement a new embedding class GNNEmbedding that uses a GCN to generate embeddings. 2. Replace the SinusoidalEmbedding in the MLPDenoiser with GNNEmbedding. 3. Construct a k-NN graph for the data, where nodes represent data points and edges represent relationships based on proximity. 4. Train the GCN on this graph to generate embeddings for each data point. 5. Integrate these embeddings into the MLPDenoiser and train the model using the same datasets. 6. Evaluate the model by comparing performance metrics such as eval loss, KL divergence, and inference time with the baseline sinusoidal embeddings.",
        "Interestingness": 9,
        "Feasibility": 8,
        "Novelty": 9,
        "Significance": 8
    }
]